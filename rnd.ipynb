{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def clone_github_repo(github_url, local_path):\n",
    "    \"\"\"\n",
    "    Clone a GitHub repository to a local path.\n",
    "    \n",
    "    Args:\n",
    "    github_url (str): The URL of the GitHub repository to clone.\n",
    "    local_path (str): The local path where the repository should be cloned.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing a boolean indicating success or failure, and a string message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['git', 'clone', github_url, local_path], \n",
    "                                check=True, \n",
    "                                capture_output=True, \n",
    "                                text=True)\n",
    "        return True, f\"Repository cloned successfully to {local_path}\"\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return False, f\"Failed to clone repository: {e.stderr.strip()}\"\n",
    "    except Exception as e:\n",
    "        return False, f\"An error occurred: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'Repository cloned successfully to C:/learning/autonomous_github/clone')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone_github_repo(\"https://github.com/jayrodge/Multimodal-RAG-with-Llama-3.2.git\", \"C:/learning/autonomous_github/clone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-text indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full-text indexing is efficient for keyword-based searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import subprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "from langchain.document_loaders import DirectoryLoader, NotebookLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "# from utils import clean_and_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_github_repo(github_url, local_path):\n",
    "    try:\n",
    "        subprocess.run(['git', 'clone', github_url, local_path], check=True)\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to clone repository: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_index_files(repo_path):\n",
    "    extensions = ['txt', 'md', 'markdown', 'rst', 'py', 'js', 'java', 'c', 'cpp', 'cs', 'go', 'rb', 'php', 'scala', 'html', 'htm', 'xml', 'json', 'yaml', 'yml', 'ini', 'toml', 'cfg', 'conf', 'sh', 'bash', 'css', 'scss', 'sql', 'gitignore', 'dockerignore', 'editorconfig', 'ipynb']\n",
    "\n",
    "    file_type_counts = {}\n",
    "    documents_dict = {}\n",
    "\n",
    "    for ext in extensions:\n",
    "        glob_pattern = f'**/*.{ext}'\n",
    "        try:\n",
    "            loader = None\n",
    "            if ext == 'ipynb':\n",
    "                loader = NotebookLoader(str(repo_path), include_outputs=True, max_output_length=20, remove_newline=True)\n",
    "            else:\n",
    "                loader = DirectoryLoader(repo_path, glob=glob_pattern)\n",
    "\n",
    "            loaded_documents = loader.load() if callable(loader.load) else []\n",
    "            if loaded_documents:\n",
    "                file_type_counts[ext] = len(loaded_documents)\n",
    "                for doc in loaded_documents:\n",
    "                    file_path = doc.metadata['source']\n",
    "                    relative_path = os.path.relpath(file_path, repo_path)\n",
    "                    file_id = str(uuid.uuid4())\n",
    "                    doc.metadata['source'] = relative_path\n",
    "                    doc.metadata['file_id'] = file_id\n",
    "\n",
    "                    documents_dict[file_id] = doc\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading files with pattern '{glob_pattern}': {e}\")\n",
    "            continue\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\n",
    "\n",
    "    split_documents = []\n",
    "    for file_id, original_doc in documents_dict.items():\n",
    "        split_docs = text_splitter.split_documents([original_doc])\n",
    "        for split_doc in split_docs:\n",
    "            split_doc.metadata['file_id'] = original_doc.metadata['file_id']\n",
    "            split_doc.metadata['source'] = original_doc.metadata['source']\n",
    "\n",
    "        split_documents.extend(split_docs)\n",
    "\n",
    "    index = None\n",
    "    if split_documents:\n",
    "        tokenized_documents = [clean_and_tokenize(doc.page_content) for doc in split_documents]\n",
    "        index = BM25Okapi(tokenized_documents)\n",
    "    return index, split_documents, file_type_counts, [doc.metadata['source'] for doc in split_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter\n",
    "\n",
    "def load_documents(repo_path):\n",
    "    extensions = [\n",
    "        'txt', 'md', 'markdown', 'rst', 'py', 'js', 'java', 'c', 'cpp', \n",
    "        'cs', 'go', 'rb', 'php', 'scala', 'html', 'htm', 'xml', 'json', \n",
    "        'yaml', 'yml', 'ini', 'toml', 'cfg', 'conf', 'sh', 'bash', 'css', \n",
    "        'scss', 'sql', 'gitignore', 'dockerignore', 'editorconfig', 'ipynb'\n",
    "    ]\n",
    "\n",
    "    file_type_counts = {}\n",
    "    documents_dict = {}\n",
    "\n",
    "    # Use tqdm.notebook for Jupyter compatibility\n",
    "    for ext in tqdm(extensions, desc=\"Processing extensions\"):\n",
    "        glob_pattern = f'**/*.{ext}'\n",
    "        try:\n",
    "            loader = None\n",
    "            if ext == 'ipynb':\n",
    "                loader = NotebookLoader(\n",
    "                    str(repo_path),\n",
    "                    include_outputs=True,\n",
    "                    max_output_length=20,\n",
    "                    remove_newline=True\n",
    "                )\n",
    "            else:\n",
    "                loader = DirectoryLoader(repo_path, glob=glob_pattern)\n",
    "\n",
    "            loaded_documents = loader.load() if callable(loader.load) else []\n",
    "            if loaded_documents:\n",
    "                file_type_counts[ext] = len(loaded_documents)\n",
    "                for doc in loaded_documents:\n",
    "                    file_path = doc.metadata['source']\n",
    "                    relative_path = os.path.relpath(file_path, repo_path)\n",
    "                    file_id = str(uuid.uuid4())\n",
    "                    doc.metadata['source'] = relative_path\n",
    "                    doc.metadata['file_id'] = file_id\n",
    "                    documents_dict[file_id] = doc\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading files with pattern '{glob_pattern}': {e}\")\n",
    "            continue\n",
    "\n",
    "    return file_type_counts, documents_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_documents(documents_dict):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=3000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "\n",
    "    split_documents = []\n",
    "    for file_id, original_doc in documents_dict.items():\n",
    "        split_docs = text_splitter.split_documents([original_doc])\n",
    "        for split_doc in split_docs:\n",
    "            split_doc.metadata['file_id'] = original_doc.metadata['file_id']\n",
    "            split_doc.metadata['source'] = original_doc.metadata['source']\n",
    "        split_documents.extend(split_docs)\n",
    "\n",
    "    index = None\n",
    "    if split_documents:\n",
    "        tokenized_documents = [clean_and_tokenize(doc.page_content) for doc in split_documents]\n",
    "        index = BM25Okapi(tokenized_documents)\n",
    "\n",
    "    document_sources = [doc.metadata['source'] for doc in split_documents]\n",
    "    \n",
    "    return index, split_documents, document_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_type_counts, documents_dict = load_documents(repo_path)\n",
    "# index, split_documents, document_sources = index_documents(documents_dict)\n",
    "# return index, split_documents, file_type_counts, document_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\67830\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\b(?:http|ftp)s?://\\S+', '', text)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query, index, documents, n_results=5):\n",
    "    query_tokens = clean_and_tokenize(query)\n",
    "    bm25_scores = index.get_scores(query_tokens)\n",
    "\n",
    "    # Compute TF-IDF scores\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=clean_and_tokenize, lowercase=True, stop_words='english', use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([doc.page_content for doc in documents])\n",
    "    query_tfidf = tfidf_vectorizer.transform([query])\n",
    "\n",
    "    # Compute Cosine Similarity scores\n",
    "    cosine_sim_scores = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
    "\n",
    "    # Combine BM25 and Cosine Similarity scores\n",
    "    combined_scores = bm25_scores * 0.5 + cosine_sim_scores * 0.5\n",
    "\n",
    "    # Get unique top documents\n",
    "    unique_top_document_indices = list(set(combined_scores.argsort()[::-1]))[:n_results]\n",
    "\n",
    "    return [documents[i] for i in unique_top_document_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone_github_repo(\"https://github.com/jayrodge/Multimodal-RAG-with-Llama-3.2\", \"multimodal-RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b02aee69244d88811872fc59595a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing extensions:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error loading file multimodal-RAG\\requirements.txt\n",
      "Error loading file multimodal-RAG\\README.md\n",
      "Error loading file multimodal-RAG\\app.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading files with pattern '**/*.txt': b\"cannot read magic file `/usr/share/misc/magic' (Bad address)\"\n",
      "Error loading files with pattern '**/*.md': b\"cannot read magic file `/usr/share/misc/magic' (Bad address)\"\n",
      "Error loading files with pattern '**/*.py': b\"cannot read magic file `/usr/share/misc/magic' (Bad address)\"\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "file_type_counts, documents_dict = load_documents(\"multimodal-RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-magic in c:\\users\\67830\\appdata\\local\\anaconda3\\envs\\bni_new\\lib\\site-packages (0.4.27)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-magic --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector indexing is a technique where vectors (often high-dimensional representations of data) are stored in a way that allows for fast similarity searches. Instead of using exact matching like in traditional full-text search, vector indexing allows you to search for approximate or nearest neighbor matches based on a distance metric, such as cosine similarity or Euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Vector Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify the type of file in github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function to classify files and exclude hidden/git-related files\n",
    "def classify_local_files(repo_path):\n",
    "    categorized_files = {\"code\": [], \"docs\": [], \"config\": []}\n",
    "    exclude_patterns = ['.git', '.sample', 'HEAD']  # Add any unwanted patterns\n",
    "\n",
    "    # Walk through all files in the local repo directory\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        # Exclude .git folder from the walk\n",
    "        dirs[:] = [d for d in dirs if d not in ['.git']]\n",
    "        \n",
    "        for file_name in files:\n",
    "            # Skip hidden and unwanted files based on patterns\n",
    "            if any(pattern in file_name for pattern in exclude_patterns):\n",
    "                continue\n",
    "            \n",
    "            # Get the full path to the file\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            \n",
    "            # Get file extension\n",
    "            _, ext = os.path.splitext(file_name)\n",
    "            \n",
    "            # Classify based on extension\n",
    "            if ext in ['.py', '.js', '.java', '.cpp']:\n",
    "                categorized_files[\"code\"].append(file_name)\n",
    "            elif ext in ['.md', '.txt', '.rst']:\n",
    "                categorized_files[\"docs\"].append(file_name)\n",
    "            elif file_name == 'requirements.txt' or ext in ['.yaml', '.json']:\n",
    "                categorized_files[\"config\"].append(file_name)\n",
    "    \n",
    "    return categorized_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': ['app.py', 'document_processors.py', 'utils.py'], 'docs': ['README.md', 'requirements.txt'], 'config': []}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "repo_path = \"clone\"\n",
    "categorized_files = classify_local_files(repo_path)\n",
    "\n",
    "print(categorized_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Graphcodebert for code type file (.py,.js, .cpp etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\67830\\AppData\\Local\\anaconda3\\envs\\BNI_NEW\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e21958354db40e9993d58afa8dfed98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\67830\\AppData\\Local\\anaconda3\\envs\\BNI_NEW\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\67830\\.cache\\huggingface\\hub\\models--microsoft--graphcodebert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9bfce60fa149af8453e6ef0c6d9026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0366fbe51e6949edb3cebeb122197cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1611778a9cd046a2af781f5e66a2b54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abba5df4b340426f8ced95da40359712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/539 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2017d2292184101846ba98addb3098c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Load GraphCodeBERT model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "\n",
    "# Function to generate embeddings\n",
    "def get_graphcode_embedding(code_snippet):\n",
    "    inputs = tokenizer(code_snippet, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).detach()  # Mean pooling to get the embedding\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4bfd4eb370e47e1921c62c0a3ffddf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\67830\\AppData\\Local\\anaconda3\\envs\\BNI_NEW\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\67830\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9be3275770b42a1b78b3f6222145be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d563dc774e447c885df6757f7e2d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2377d54aca54ee987beb24f013db5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\67830\\AppData\\Local\\anaconda3\\envs\\BNI_NEW\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ad763989ff49fb99018cd3401868cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb9869571f04cfa9942271c25921155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bf94c086b542158f64196c9c0d813e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585a51a0f0154a6d9dc31247c3818e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0723c294d8416586db6c3d078857cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6099700281459cb0da5b42a496f9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d5496b0b0649958ae4d68dd09b4704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "text_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Function to generate embeddings for text files\n",
    "def generate_text_embedding(text_snippet):\n",
    "    return text_model.encode(text_snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vectors_for_repo(repo_path, categorized_files):\n",
    "    vectors = {\"code\": [], \"docs\": [], \"config\": []}\n",
    "\n",
    "    # Process code files\n",
    "    for code_file in categorized_files[\"code\"]:\n",
    "        file_path = os.path.join(repo_path, code_file)\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            code_content = f.read()\n",
    "            code_embedding = get_graphcode_embedding(code_content)\n",
    "            vectors[\"code\"].append((code_file, code_embedding))\n",
    "\n",
    "    # Process documentation files\n",
    "    for doc_file in categorized_files[\"docs\"]:\n",
    "        file_path = os.path.join(repo_path, doc_file)\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            doc_content = f.read()\n",
    "            doc_embedding = generate_text_embedding(doc_content)\n",
    "            vectors[\"docs\"].append((doc_file, doc_embedding))\n",
    "\n",
    "    # Process configuration files\n",
    "    for config_file in categorized_files[\"config\"]:\n",
    "        file_path = os.path.join(repo_path, config_file)\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            config_content = f.read()\n",
    "            config_embedding = generate_text_embedding(config_content)\n",
    "            vectors[\"config\"].append((config_file, config_embedding))\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: code\n",
      "File: app.py, Embedding shape: torch.Size([1, 768])\n",
      "File: document_processors.py, Embedding shape: torch.Size([1, 768])\n",
      "File: utils.py, Embedding shape: torch.Size([1, 768])\n",
      "Category: docs\n",
      "File: README.md, Embedding shape: (384,)\n",
      "File: requirements.txt, Embedding shape: (384,)\n",
      "Category: config\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "repo_path = \"clone\"\n",
    "\n",
    "# Assuming you have categorized the files using the previous classify_local_files function\n",
    "categorized_files = classify_local_files(repo_path)\n",
    "\n",
    "# Generate vectors\n",
    "vectors = generate_vectors_for_repo(repo_path, categorized_files)\n",
    "\n",
    "# Print the vectors for each file type\n",
    "for category, file_vectors in vectors.items():\n",
    "    print(f\"Category: {category}\")\n",
    "    for file_name, embedding in file_vectors:\n",
    "        print(f\"File: {file_name}, Embedding shape: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Embedding in Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Category**  | **Vector Database**  | **Key Features**                                               | **Scalability**                          | **Best For**                                 |\n",
    "|---------------|----------------------|----------------------------------------------------------------|------------------------------------------|----------------------------------------------|\n",
    "| **Cloud-Based**  | **Pinecone**         | Managed service, high performance, real-time & batch updates    | Automatically scalable on the cloud      | Large-scale enterprise applications          |\n",
    "|                 | **Weaviate (Cloud)** | AI-first, hybrid search (vector + keyword), managed cloud       | Automatically scalable                   | AI-powered apps needing hybrid search        |\n",
    "|                 | **Qdrant Cloud**     | Real-time, high-throughput, low-latency similarity search       | Highly scalable on the cloud             | Real-time recommendation systems             |\n",
    "|                 | **Vearch Cloud**     | Multi-modal search (text, image, etc.), fully managed           | Cloud-native scalability                 | Multi-modal data search                      |\n",
    "| **Self-Hosted**  | **FAISS**            | Local, high-performance vector search, indexing algorithms      | Small/medium-scale (custom distributed possible) | Fast, customizable, local vector search       |\n",
    "|                 | **Chroma DB**           | Distributed, multi-modal search, integrates with big data tools | Highly scalable (Kubernetes integration) | Large-scale, multi-modal, distributed search |\n",
    "|                 | **Qdrant (Self-hosted)** | Open-source, real-time, RESTful API                             | Moderate-scale, extendable to distributed setups | Moderate-scale, real-time vector search       |\n",
    "|                 | **Weaviate (Self-hosted)** | Hybrid vector + keyword search, schema-based data storage       | Suitable for medium/large projects       | Complex hybrid search apps                   |\n",
    "|                 | **Vespa**            | Large-scale enterprise-grade vector + text retrieval            | Highly scalable                          | Large enterprises, recommendation systems    |\n",
    "|                 | **Elasticsearch (k-NN Plugin)** | Combines keyword + vector search, open-source                   | Highly scalable                          | Text-heavy apps needing hybrid search        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "class CodeVectorIndexer:\n",
    "    def __init__(self, model_name='microsoft/codebert-base'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.index = None\n",
    "        self.documents = []\n",
    "\n",
    "    def encode(self, texts, batch_size=32):\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # Use [CLS] token embedding\n",
    "            all_embeddings.append(embeddings)\n",
    "        return np.vstack(all_embeddings)\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        self.documents.extend(documents)\n",
    "        embeddings = self.encode(documents)\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "\n",
    "    def search(self, query, k=5):\n",
    "        query_vector = self.encode([query]).astype('float32')\n",
    "        distances, indices = self.index.search(query_vector, k)\n",
    "        return [(self.documents[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    indexer = CodeVectorIndexer()\n",
    "\n",
    "    # Simulating content from GitHub repos\n",
    "    documents = [\n",
    "        \"def train_model(data, labels):\\n    model = RandomForestClassifier()\\n    model.fit(data, labels)\\n    return model\",\n",
    "        \"class NeuralNetwork(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.fc1 = nn.Linear(784, 128)\\n        self.fc2 = nn.Linear(128, 10)\",\n",
    "        \"def preprocess_text(text):\\n    tokens = word_tokenize(text.lower())\\n    return [token for token in tokens if token not in stop_words]\",\n",
    "        \"async def fetch_data(url):\\n    async with aiohttp.ClientSession() as session:\\n        async with session.get(url) as response:\\n            return await response.json()\",\n",
    "        \"SELECT repository_name, COUNT(*) as star_count\\nFROM github_stars\\nGROUP BY repository_name\\nORDER BY star_count DESC\\nLIMIT 10;\"\n",
    "    ]\n",
    "\n",
    "    indexer.add_documents(documents)\n",
    "\n",
    "    # Perform a search\n",
    "    query = \"machine learning model training\"\n",
    "    results = indexer.search(query)\n",
    "    print(f\"Search results for '{query}':\")\n",
    "    for doc, score in results:\n",
    "        print(f\"Score: {score:.4f}\\nDocument: {doc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BNI_NEW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
